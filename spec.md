Building a **Computer Vision for Property Assessment** prototype is an excellent choice for an Opendoor-aligned project in February 2026. It directly ties into their AI-enhanced in-home assessments (now increasingly augmented by generative and analytical AI tools), instant-offer pricing models, and efforts to reduce manual Key Agent workload through better condition insights. The goal: user uploads 1â€“5 home photos (kitchen, bathroom, living room, exterior/yard), the app detects key features/condition indicators, and outputs a suggested price adjustment (e.g., +$15k for modern kitchen, -$8k for needed repairs like outdated cabinets or visible wear).

This is feasible as a weekend-to-1-month project using mostly pre-trained models via Hugging Face, with minimal custom training.

### Recommended Approach & Architecture
Use a **multi-stage pipeline** (practical and explainable):
1. **Room / Scene Classification** â†’ Identify what the photo shows (kitchen, bathroom, bedroom, yard, facade).
2. **Object Detection / Semantic Segmentation** â†’ Find relevant elements (cabinets, appliances, countertops, cracks, outdated fixtures, yard size proxy via grass/trees/fence detection).
3. **Condition / Quality Scoring** â†’ Rule-based or lightweight classifier on detected features (e.g., stainless steel appliances = upgrade; visible cracks/peeling paint = repair needed).
4. **Price Adjustment Suggestion** â†’ Simple heuristic mapping (e.g., modern kitchen +5â€“10% value uplift; major repairs -3â€“8%) or regressor if you add a small dataset.

**Streamlit** app flow:
- Upload photo(s)
- Display analyzed image with bounding boxes / highlights
- Show detected features + condition score
- Output: "Estimated adjustment: +$12,000 (premium kitchen features detected; minor wear on flooring)"

### Best Pre-trained Models from Hugging Face (2026 Landscape)
Focus on accessible, high-performance optionsâ€”no heavy fine-tuning required initially.

- **Room Classification** (quick first step):
  - `andupets/real-estate-image-classification` â€” Specifically trained on real-estate photos: kitchen, bedroom, bathroom, living room, house facade, etc. Lightweight and directly relevant.
  - `JuanMa360/room-classification` or `Tater86/room-classifier-v1` â€” Similar interior/exterior room classifiers.
  - General alternative: Use `google/vit-base-patch16-224` (Vision Transformer) + simple fine-tune if needed, but the real-estate ones save time.

- **Interior Scene Understanding / Segmentation**:
  - UperNetForSemanticSegmentation models (e.g., from Hugging Face hub like `openmmlab/upernet-convnext-small`) â€” Often used in interior design pipelines for segmenting walls, floors, cabinets, appliances.
  - ControlNet-related models (e.g., `BertChristiaens/controlnet-seg-room`) â€” Trained with segmentation maps on interior rooms; great for understanding layout/elements even if your primary goal isn't generation.

- **Object Detection for Specific Features** (repairs, quality):
  - General-purpose: `facebook/detr-resnet-50` or Ultralytics YOLOv8/v10 via Hugging Face integration â€” Detect common objects (fridge, oven, sink, window, door) then apply rules (e.g., old fridge style â†’ deduct points).
  - For defects/condition: Transfer-learn from construction defect models (e.g., crack/stain detection papers use VGG-16/ResNet-50 pre-trained on ImageNet + fine-tune on small labeled sets). No perfect off-the-shelf "home repair detector," but you can start with general anomaly detection or prompt-based with multimodal models.

- **Multimodal / Zero-Shot Option** (easiest for MVP if you want minimal code):
  - Use vision-language models like CLIP (`openai/clip-vit-large-patch14`) or newer 2026 multimodal (e.g., LLaVA variants on HF) with prompts: "Describe the kitchen quality: modern/upgraded, outdated, needs repairs, visible defects like cracks/peeling."
  - Or Molmo-style open models for detailed captioning + parsing.

Start simple: room classifier â†’ object detection â†’ hardcoded rules for adjustment.

### Datasets to Bootstrap or Evaluate
- Real-estate specific: `andupets/real-estate-image-classification` has example images; `zillow/real_estate_v1` (text-focused but pairable).
- Indoor objects: HomeObjects-3K (~3k household items, good for appliance/furniture detection).
- General indoor: InteriorNet (huge, but older); ADE20K or MIT Indoor Scenes for segmentation.
- For condition: Small custom setâ€”label 50â€“200 photos yourself (or from Kaggle housing images) for "needs repair" vs "good condition."

### Implementation Steps (Streamlit + HF)
```python
# app.py (Streamlit)
import streamlit as st
from transformers import pipeline, AutoImageProcessor, AutoModelForImageClassification
from PIL import Image
import torch

st.title("AI Property Assessment - Opendoor Style")

uploaded_file = st.file_uploader("Upload home photo (kitchen, bath, yard...)", type=["jpg", "png"])

if uploaded_file:
    image = Image.open(uploaded_file)
    st.image(image, caption="Uploaded Photo", use_column_width=True)

    # 1. Room classification
    classifier = pipeline("image-classification", model="andupets/real-estate-image-classification")
    room_result = classifier(image)[0]
    st.write(f"Detected room: **{room_result['label']}** (confidence: {room_result['score']:.2%})")

    # 2. Example: simple quality heuristic (expand with detection)
    if "kitchen" in room_result['label'].lower():
        # Placeholder: in real version run object detection for appliances/cabinets
        adjustment = "+$8,000 â€“ $15,000 (modern features likely)"
        st.success(adjustment)
    elif "exterior" in room_result['label'].lower():
        adjustment = "-$5,000 if yard overgrowth or facade wear detected"
        st.info(adjustment)
    else:
        st.write("Add more logic for this room type!")

    # Bonus: add defect prompt with multimodal if you integrate LLaVA/CLIP
```

### Tips to Make It Stand Out for Opendoor Application
- **Quantify**: Test on 50â€“100 public listing photos (Redfin/Zillow scrapes or Kaggle); report "Detected kitchen upgrades in 82% of premium listings" or "Flagged repair needs with 75% agreement vs manual review."
- **Opendoor Tie-in**: "Inspired by Opendoor's AI-driven assessments and RiskAIâ€”prototype automates condition flags to refine instant offers faster."
- **Ethics/Bias**: Note limitations (lighting/angle bias, no true "repair cost" without training data) and suggest production would use diverse datasets.
- **Next Level (if time)**: Add YOLO for bounding boxes on appliances â†’ map to quality tiers; integrate simple regression on tabular home features + CV score.
- **Deployment**: Hugging Face Spaces (free GPU for inference) + Streamlit â€” share live link in your DM/application.

This shows end-to-end ML thinking, domain relevance, and ship-it attitude. If you share your strongest area (e.g., comfortable with PyTorch? Prefer no-code first?), I can point to a starter repo (many Streamlit + HF CV demos exist) or refine the pipeline! ðŸš€

- - 

Here is a complete **A-Z guide** you can hand directly to your AI coder (or use yourself) to build the **Computer Vision for Property Assessment** prototype as a Streamlit web app.

This guide targets a realistic MVP in February 2026: users upload 1â€“5 home photos â†’ app classifies the room type â†’ detects key objects/features â†’ applies simple heuristics/rules to estimate condition/quality â†’ suggests a price adjustment (Â±$ amount or percentage uplift/deduction). It aligns with Opendoor-style instant-offer condition assessments.

### Project Goals & Scope (MVP)
- Input: User uploads JPG/PNG photos (kitchen, bathroom, living room, bedroom, exterior/yard focus)
- Output:
  - Detected room type (e.g., "kitchen") + confidence
  - Key detected features (e.g., "stainless steel appliances", "granite counters", "cracks/peeling paint", "outdated cabinets")
  - Condition score (e.g., 1â€“10 or "Premium / Good / Fair / Needs Repair")
  - Suggested price adjustment (e.g., "+$12,000 â€“ modern upgraded kitchen" or "âˆ’$7,500 â€“ visible wear & outdated fixtures")
- Nice-to-have visuals: Annotated image with bounding boxes or highlights (if using detection)
- Deployment target: Hugging Face Spaces or Streamlit Community Cloud (free tier)

Time estimate for experienced coder: 10â€“25 hours (mostly wiring models + rules).

### 1. Tech Stack (2026 Recommended Versions)
- Python 3.10â€“3.12
- Streamlit 1.42+ (`pip install streamlit`)
- Transformers 4.48+ (`pip install transformers`)
- Torch 2.4+ or 2.5 (with CUDA if you have GPU; CPU works for demo)
- Pillow (PIL) for image handling
- OpenCV (`pip install opencv-python`) â€“ optional but helpful for drawing boxes
- Optional extras: timm, ultralytics (for YOLOv8/v10 if you go detection-heavy)

No heavy training needed for MVP â€” rely on pre-trained models.

### 2. Core Models from Hugging Face (Pick & Combine)
Use these realistic 2025â€“2026 options:

**A. Room / Scene Classification** (First stage â€“ must-have)
- Primary: `JuanMa360/room-classification` â†’ classes include kitchen, bathroom, bedroom, living_room, exterior, closets, others
- Strong alternative: `andupets/real-estate-image-classification` or its 30-class variant (`andupets/real-estate-image-classification-30classes`) â†’ includes yard, kitchen, etc.
- Fallback general: `google/vit-base-patch16-224` (fine-tune lightly if needed, but not for MVP)

**B. Object Detection / Feature Detection** (Second stage â€“ quality differentiator)
- Best off-the-shelf general: `facebook/detr-resnet-50` or `microsoft/conditional-detr-resnet-50` (COCO classes â†’ detect fridge, oven, sink, couch, bed, window, etc.)
- Modern & faster: Use Ultralytics YOLOv8/v10 via `ultralytics` package (not pure HF, but integrates easily) â€“ pre-trained on COCO, detect appliances/furniture
- Defect/condition specific: No perfect public model exists for "home repair needed". Options:
  - Zero-shot / prompt-based: Use multimodal like `llava-hf/llava-1.5-7b-hf` or `openai/clip-vit-large-patch14` with prompts ("Does this kitchen show signs of needed repairs like cracks, peeling paint, outdated appliances? Describe quality.")
  - Rule-based on detected objects (recommended for MVP): If old fridge style or missing modern appliances â†’ deduct points

**C. Optional Multimodal Captioning** (for richer description without detection)
- `Salesforce/blip-image-captioning-large` or `nlpconnect/vit-gpt2-image-captioning`
- Newer 2026-friendly: LLaVA variants (`llava-hf/llava-v1.6-mistral-7b-hf`) â€“ prompt: "Describe the condition and quality of this [room type] in detail, focusing on upgrades, wear, and repair needs."

Start with classification + rule-based heuristics â†’ add detection or multimodal later.

### 3. Architecture / Pipeline
1. Upload image(s)
2. Preprocess (resize to 224Ã—224 or model-native size)
3. Run room classification â†’ get top label (kitchen / bathroom / etc.)
4. Conditional logic per room:
   - Kitchen: look for appliances, counters, cabinets
   - Bathroom: fixtures, tiles, mold signs
   - Exterior/Yard: grass, fence, siding condition
5. Feature extraction:
   - Option A (simple): Run object detection â†’ count/match expected modern items (stainless fridge, double oven, etc.)
   - Option B (easier): Use BLIP/LLAVA caption â†’ parse keywords (modern, outdated, damaged, premium)
6. Score condition (0â€“10):
   - Heuristic table (customize!):
     - Modern appliances + clean: +8â€“10
     - Outdated but functional: 5â€“7
     - Visible damage (cracks/peeling/old): 2â€“4
7. Price adjustment mapping (very approximate â€“ for demo only):
   - Kitchen upgrade: +4â€“12% (~$8kâ€“$25k on median home)
   - Bathroom modern: +3â€“8%
   - Repair needs: âˆ’2â€“10%
   - Display range + disclaimer: "Illustrative only â€“ not financial advice"

### 4. Full Starter Code Structure (app.py)

```python
import streamlit as st
from transformers import pipeline, AutoImageProcessor, AutoModelForImageClassification
from PIL import Image
import io
import torch

# Cache models
@st.cache_resource
def load_room_classifier():
    return pipeline("image-classification", model="JuanMa360/room-classification")

@st.cache_resource
def load_captioner():
    return pipeline("image-to-text", model="Salesforce/blip-image-captioning-large")

room_classifier = load_room_classifier()
captioner = load_captioner()

st.title("AI Property Condition Assessment â€“ Opendoor-Inspired MVP")
st.markdown("Upload 1â€“5 photos of rooms or exterior. Get room type, condition insights, and illustrative price adjustment.")

uploaded_files = st.file_uploader("Upload home photos", type=["jpg", "jpeg", "png"], accept_multiple_files=True)

if uploaded_files:
    for idx, uploaded_file in enumerate(uploaded_files):
        image = Image.open(uploaded_file)
        st.image(image, caption=f"Photo {idx+1}", use_column_width=True)

        # 1. Room classification
        try:
            results = room_classifier(image)
            top_label = results[0]['label']
            top_score = results[0]['score']
            st.subheader(f"Detected: **{top_label}** (confidence: {top_score:.1%})")
        except Exception as e:
            st.error(f"Classification failed: {e}")
            continue

        # 2. Caption / description
        try:
            caption = captioner(image, max_new_tokens=80)[0]['generated_text']
            st.write("**Description:** " + caption)
        except:
            st.warning("Captioning skipped.")

        # 3. Simple heuristic adjustment (expand heavily!)
        adjustment = 0
        notes = []

        if "kitchen" in top_label.lower():
            if "stainless" in caption.lower() or "modern" in caption.lower():
                adjustment += 12000
                notes.append("Premium/modern kitchen features detected â†’ potential uplift")
            if "outdated" in caption.lower() or "old" in caption.lower():
                adjustment -= 6000
                notes.append("Outdated elements noted â†’ possible deduction")

        elif "bathroom" in top_label.lower():
            if "tile" in caption.lower() and "modern" in caption.lower():
                adjustment += 8000
            else:
                adjustment -= 4000

        elif "exterior" in top_label.lower() or "yard" in top_label.lower():
            if "well-maintained" in caption.lower():
                adjustment += 5000
            if "damage" in caption.lower() or "crack" in caption.lower():
                adjustment -= 7000

        if adjustment > 0:
            st.success(f"Suggested adjustment: **+${adjustment:,}** (illustrative only)")
        elif adjustment < 0:
            st.warning(f"Suggested adjustment: **${adjustment:,}** (illustrative only)")
        else:
            st.info("Neutral adjustment â€“ add more logic or photos!")

        st.markdown("---")

st.markdown("""
### Disclaimers
- This is a prototype â€“ adjustments are rule-based heuristics, **not accurate valuations**.
- Inspired by Opendoor's AI-enhanced assessments.
- Improve with: YOLO detection, fine-tuned defect model, or multimodal LLM parsing.
""")
```

### 5. Next Iterations (After MVP Works)
- Add YOLOv8: `from ultralytics import YOLO; model = YOLO("yolov8n.pt")` â†’ draw boxes on image
- Use LLaVA for better condition prompts: "Assess upgrade level and repair needs on 1â€“10 scale"
- Small fine-tune: Collect ~200 labeled photos (good/poor kitchen etc.) â†’ LoRA on ViT
- Multi-photo aggregation: Average scores across uploads
- Add sliders: User inputs zip code / home value â†’ scale adjustment %
- Export PDF report

### 6. Deployment & Polish
- `streamlit run app.py`
- Deploy â†’ Hugging Face Spaces (upload repo, enable GPU if using large models)
- GitHub repo structure:
  ```
  â”œâ”€â”€ app.py
  â”œâ”€â”€ requirements.txt
  â”œâ”€â”€ README.md           # Explain Opendoor tie-in, metrics, limitations
  â””â”€â”€ .streamlit/         # config.toml for theme
  ```
- requirements.txt example:
  ```
  streamlit
  transformers
  torch
  pillow
  ```

### 7. Testing & Validation Tips
- Test images: Google "modern kitchen", "outdated bathroom", "damaged exterior" â†’ check if logic triggers correctly
- Metrics to track (manual): % correct room type, sensible adjustment direction
- Edge cases: Blurry photos, weird angles, non-room shots

This gets you a solid, demo-able MVP that shows CV + real-estate domain understanding. Hand this guide to your coder â€” they can start coding immediately. If they hit model availability issues (rare in 2026), swap to the next-best listed model.

Let me know if you want expansions (YOLO integration code, LLaVA prompt examples, etc.)! ðŸš€

- - 

Here is the **expanded A-Z guide** for your AI coder to build the full-featured **Computer Vision for Property Assessment** prototype (Streamlit app). This incorporates the expansions you requested: 

- **YOLO integration** for object detection (bounding boxes on key features like appliances, cabinets, cracks, etc.)
- **LLaVA (multimodal LLM)** for richer, prompt-based condition descriptions and quality scoring
- **Advanced heuristics** with better price adjustment logic
- **Multi-photo support** + aggregation
- **Visual annotations** (drawing boxes/highlights on images)
- **Small fine-tune notes** (optional extension)
- **Deployment & polish** tips updated for 2026

The MVP remains achievable in 10â€“30 hours, but expansions push it toward a more impressive demo (closer to production-like quality for an Opendoor application showcase).

### Updated Tech Stack (Feb 2026)
- Python 3.11â€“3.12
- `streamlit` 1.42+
- `transformers` 4.48+
- `torch` 2.5+ (CUDA recommended for speed; CPU fallback ok)
- `pillow`, `opencv-python` (for drawing)
- `ultralytics` (for YOLOv8/v10/v11/YOLO26 family â€” Ultralytics now at YOLO26 as of Jan 2026)
- Optional heavy hitter: `llava-hf/llava-v1.6-mistral-7b-hf` or `llava-hf/llava-1.5-7b-hf` (lighter) for multimodal reasoning

Install:
```bash
pip install streamlit transformers torch pillow opencv-python ultralytics
```

### Core Models (2026 Landscape)
- **Room Classification** â€” `andupets/real-estate-image-classification` (kitchen, bathroom, bedroom, living room, facade, yard, etc.) or `JuanMa360/room-classification` or the 30-class variant `andupets/real-estate-image-classification-30classes`
- **Object Detection** â€” Ultralytics YOLO26 (or YOLOv8/YOLOv10 if lighter needed): `yolo26n.pt` / `yolov8n.pt` (pre-trained on COCO â†’ detect fridge, oven, sink, couch, window, etc.)
- **Multimodal Description & Scoring** â€” LLaVA family: `llava-hf/llava-v1.6-mistral-7b-hf` (strong reasoning/OCR) or lighter `llava-hf/llava-1.5-7b-hf`. Prompt for condition: "Assess this [room] on a 1â€“10 quality scale. Note upgrades (modern appliances, granite), wear (outdated, peeling), repairs needed (cracks, damage). Be detailed."
- **Defect Helpers** (if needed): `cazzz307/yolov8-crack-detection` or `OpenSistemas/YOLOv8-crack-seg` for cracks/walls (niche but useful for exterior/interior wear)

### Full Expanded Pipeline
1. Upload 1â€“5 photos
2. Classify room type per photo
3. Run YOLO detection â†’ annotate image with boxes (e.g., label appliances)
4. Run LLaVA on photo with room-aware prompt â†’ get detailed description + quality score
5. Aggregate: Average scores, combine notes across photos
6. Apply enhanced heuristics â†’ price adjustment range
7. Display annotated images + summary

### Full Expanded Code (app.py)

```python
import streamlit as st
from transformers import pipeline, AutoProcessor, LlavaForConditionalGeneration
from PIL import Image, ImageDraw, ImageFont
import torch
import cv2
import numpy as np
from ultralytics import YOLO
import io

# ---------------- Cache heavy models ----------------
@st.cache_resource
def load_room_classifier():
    return pipeline("image-classification", model="andupets/real-estate-image-classification")

@st.cache_resource
def load_yolo():
    return YOLO("yolo26n.pt")  # or "yolov8n.pt" if lighter needed

@st.cache_resource
def load_llava():
    model_id = "llava-hf/llava-v1.6-mistral-7b-hf"  # or llava-1.5-7b-hf for speed
    processor = AutoProcessor.from_pretrained(model_id)
    model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")
    return processor, model

room_classifier = load_room_classifier()
yolo_model = load_yolo()
llava_processor, llava_model = load_llava()

# ---------------- Heuristic mapping (customize heavily!) ----------------
def get_adjustment(room, llava_desc, yolo_results):
    base_adj = 0
    notes = []

    desc_lower = llava_desc.lower()
    # Kitchen specifics
    if "kitchen" in room.lower():
        if any(word in desc_lower for word in ["modern", "stainless", "granite", "quartz", "new", "upgraded"]):
            base_adj += 15000
            notes.append("Premium kitchen upgrades detected")
        if any(word in desc_lower for word in ["outdated", "old", "worn", "laminate", "formica"]):
            base_adj -= 8000
            notes.append("Outdated kitchen elements")
        if "crack" in desc_lower or "peel" in desc_lower:
            base_adj -= 5000
            notes.append("Repair needs (cracks/peeling)")

    # Bathroom
    elif "bathroom" in room.lower():
        if "modern" in desc_lower or "tile" in desc_lower and "new" in desc_lower:
            base_adj += 10000
        else:
            base_adj -= 6000

    # Exterior/Yard
    elif "exterior" in room.lower() or "yard" in room.lower():
        if "well-maintained" in desc_lower or "clean" in desc_lower:
            base_adj += 7000
        if "damage" in desc_lower or "overgrown" in desc_lower:
            base_adj -= 9000

    # YOLO bonus/penalty (e.g., count modern appliances)
    appliance_count = sum(1 for r in yolo_results if r.names[r.boxes.cls.item()] in ["refrigerator", "oven", "sink"])
    if appliance_count >= 3:
        base_adj += 3000  # Many appliances â†’ likely functional kitchen

    return base_adj, notes

# ---------------- Annotate image with YOLO boxes ----------------
def annotate_image(image, yolo_results):
    img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    for r in yolo_results:
        boxes = r.boxes
        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            cls = int(box.cls)
            label = yolo_model.names[cls]
            conf = float(box.conf)
            cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(img_cv, f"{label} {conf:.2f}", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    return Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))

# ---------------- Main App ----------------
st.title("AI Property Assessment â€“ Advanced Opendoor-Style Prototype")
st.markdown("Upload photos â†’ room classification + YOLO detection + LLaVA condition analysis + aggregated price adjustment suggestion.")

uploaded_files = st.file_uploader("Upload 1â€“5 home photos", type=["jpg", "jpeg", "png"], accept_multiple_files=True)

if uploaded_files:
    all_adjustments = []
    all_notes = []
    all_quality_scores = []

    for idx, file in enumerate(uploaded_files):
        image = Image.open(file)
        st.subheader(f"Photo {idx+1}")
        st.image(image, use_column_width=True)

        # 1. Room classification
        room_results = room_classifier(image)
        room = room_results[0]['label']
        conf = room_results[0]['score']
        st.write(f"**Room:** {room} (conf: {conf:.1%})")

        # 2. YOLO detection + annotate
        yolo_results = yolo_model(image)
        annotated = annotate_image(image, yolo_results)
        st.image(annotated, caption="YOLO Detection (objects boxed)", use_column_width=True)

        # 3. LLaVA multimodal analysis
        prompt = f"Assess this {room} on quality 1â€“10. Detail upgrades (modern appliances, materials), wear/outdated features, needed repairs (cracks, damage, mold). Be specific and objective."
        inputs = llava_processor(text=prompt, images=image, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
        with torch.inference_mode():
            output = llava_model.generate(**inputs, max_new_tokens=200, do_sample=False)
        llava_desc = llava_processor.decode(output[0], skip_special_tokens=True)
        st.markdown(f"**LLaVA Analysis:**\n{llava_desc}")

        # Parse simple score if present (heuristic â€“ improve with regex)
        try:
            score_line = [line for line in llava_desc.split('\n') if "1â€“10" in line or "scale" in line][0]
            quality_score = float(score_line.split()[-1]) if score_line.split()[-1].replace('.', '').isdigit() else 5.0
        except:
            quality_score = 5.0
        all_quality_scores.append(quality_score)

        # 4. Adjustment
        adj, notes = get_adjustment(room, llava_desc, yolo_results)
        all_adjustments.append(adj)
        all_notes.extend(notes)

        st.markdown("---")

    # ---------------- Aggregation ----------------
    if all_adjustments:
        avg_adj = sum(all_adjustments) / len(all_adjustments)
        avg_quality = sum(all_quality_scores) / len(all_quality_scores)
        unique_notes = list(set(all_notes))

        st.success(f"**Aggregated Suggestion:** ${avg_adj:,.0f} adjustment (avg quality: {avg_quality:.1f}/10)")
        if avg_adj > 0:
            st.success("Positive uplift likely from detected upgrades.")
        elif avg_adj < 0:
            st.warning("Potential deductions from wear/repairs.")
        st.write("**Key Notes:** " + "; ".join(unique_notes))

st.markdown("""
**Disclaimers & Next Steps**  
- Prototype only â€“ adjustments heuristic/illustrative (not real valuation).  
- LLaVA can be slow/heavy; use lighter variant or batch if needed.  
- Expand: Fine-tune YOLO on home-repair dataset (e.g., add labels for "outdated fridge").  
- Deploy: Hugging Face Spaces (GPU enabled) or Streamlit Cloud.  
Inspired by Opendoor's AI assessments â€“ shows CV + multimodal potential!
""")
```

### Extensions for Even More Polish
- **Multi-photo summary table**: Use `st.dataframe` to show per-photo room/score/adj
- **User input scaling**: Add `st.slider("Home value $", 200000, 2000000)` â†’ scale adj as % uplift
- **Small fine-tune path**: Use ~100â€“300 labeled photos (good/poor condition per room) â†’ LoRA on LLaVA or fine-tune YOLO on custom classes ("modern_kitchen", "damaged_wall")
- **Error handling**: Add try/except around LLaVA (fallback to BLIP captioning if OOM)

This version is demo-ready and impressive: shows classification, detection, multimodal reasoning, annotation, and business logic. Test with diverse photos (modern kitchen vs. dated bathroom vs. cracked exterior). If issues arise (model loading, speed), swap to lighter variants first.

Let me know if you want a requirements.txt, README template, or code for PDF export/report! ðŸš€

- - - 

Here is the **even more polished** version of the **Computer Vision for Property Assessment** Streamlit app. This iteration incorporates 2026 best practices, updated models, performance optimizations, better UX, error handling, visual polish, and production-like touches to make it truly standout for an Opendoor application showcase.

### Key Polish Upgrades (Feb 2026)
- **Updated models**:
  - Room classification: `andupets/real-estate-image-classification` (still strong; alternatives like `strollingorange/roomLuxuryAnnotater` for luxury scoring if you expand later).
  - Detection: **YOLO26n** (`yolo26n.pt`) â€“ latest Ultralytics release (Jan 2026), edge-optimized, faster CPU/GPU inference, better accuracy on everyday objects.
  - Multimodal: **llava-hf/llava-v1.6-mistral-7b-hf** (LLaVA-NeXT family) â€“ superior high-res support, OCR, reasoning for condition assessment vs older 1.5.
- **Performance**:
  - Aggressive `@st.cache_resource` for all heavy models (load once, share across sessions/users).
  - Fallbacks (e.g., lighter BLIP if LLaVA OOM on free tiers).
  - Async-like batching hints; TTL on any dynamic parts.
- **UX/Polish**:
  - Progress spinners, collapsible sections, tabs for per-photo vs aggregate view.
  - Annotated images with clearer labels, color-coded boxes (green=positive features, red=potential issues â€“ heuristic).
  - Summary dashboard with metrics table, total adjustment range.
  - Home value slider to scale adjustments realistically.
  - Export button for simple text/CSV summary.
- **Robustness**: Better error handling, device detection (CUDA/CPU), input validation.
- **Opendoor Tie-in**: Disclaimer + "how this could extend RiskAI/assessments" note.

### requirements.txt (pin versions for reproducibility)
```
streamlit==1.42.0
transformers==4.48.0
torch==2.5.0
pillow==10.4.0
opencv-python==4.11.0.86
ultralytics==8.4.9  # supports YOLO26
```

### Full Polished Code (app.py)

```python
import streamlit as st
from transformers import pipeline, AutoProcessor, LlavaForConditionalGeneration
from PIL import Image, ImageDraw
import torch
import cv2
import numpy as np
from ultralytics import YOLO
from io import BytesIO
import pandas as pd

# â”€â”€ Device setup â”€â”€
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32
st.sidebar.info(f"Running on **{device.upper()}** | dtype: {dtype}")

# â”€â”€ Cached heavy resources â”€â”€
@st.cache_resource(show_spinner="Loading room classifier...")
def load_room_classifier():
    return pipeline("image-classification", model="andupets/real-estate-image-classification", device=0 if device=="cuda" else -1)

@st.cache_resource(show_spinner="Loading YOLO26 (edge-optimized detector)...")
def load_yolo():
    return YOLO("yolo26n.pt")

@st.cache_resource(show_spinner="Loading LLaVA-NeXT (v1.6 Mistral 7B) for condition analysis...")
def load_llava():
    model_id = "llava-hf/llava-v1.6-mistral-7b-hf"
    processor = AutoProcessor.from_pretrained(model_id)
    model = LlavaForConditionalGeneration.from_pretrained(
        model_id, torch_dtype=dtype, device_map="auto", low_cpu_mem_usage=True
    )
    return processor, model

room_classifier = load_room_classifier()
yolo_model = load_yolo()
llava_processor, llava_model = load_llava()

# â”€â”€ Enhanced adjustment logic (customizable, more granular) â”€â”€
def compute_adjustment(room, llava_desc, yolo_results, base_home_value):
    desc_lower = llava_desc.lower()
    adj_pct = 0.0
    notes = []

    if "kitchen" in room.lower():
        if any(w in desc_lower for w in ["modern", "stainless", "granite", "quartz", "new", "upgraded", "premium"]):
            adj_pct += 0.06  # ~6% uplift
            notes.append("Premium kitchen upgrades")
        if any(w in desc_lower for w in ["outdated", "old", "worn", "laminate", "formica"]):
            adj_pct -= 0.04
            notes.append("Outdated kitchen")
        if any(w in desc_lower for w in ["crack", "peel", "damage", "repair"]):
            adj_pct -= 0.03
            notes.append("Repair needs detected")

    elif "bathroom" in room.lower():
        if "modern" in desc_lower or ("tile" in desc_lower and "new" in desc_lower):
            adj_pct += 0.05
        else:
            adj_pct -= 0.03

    elif any(x in room.lower() for x in ["exterior", "yard", "facade"]):
        if "well-maintained" in desc_lower or "clean" in desc_lower:
            adj_pct += 0.035
        if any(w in desc_lower for w in ["damage", "overgrown", "crack", "peel"]):
            adj_pct -= 0.045

    # YOLO signals
    modern_appliances = sum(1 for r in yolo_results if r.names[int(r.boxes.cls)] in ["refrigerator", "oven", "microwave", "sink"])
    if modern_appliances >= 3:
        adj_pct += 0.015
        notes.append("High appliance count â†’ functional/modern")

    dollar_adj = base_home_value * adj_pct
    return round(dollar_adj), round(adj_pct * 100, 1), notes

# â”€â”€ Annotate with color logic (green=good, orange=neutral, red=issue) â”€â”€
def annotate_image(image, yolo_results, llava_desc):
    draw = ImageDraw.Draw(image)
    try:
        font = ImageFont.truetype("arial.ttf", 18)  # fallback if not found
    except:
        font = ImageFont.load_default()

    desc_lower = llava_desc.lower()
    for r in yolo_results:
        for box in r.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            cls_name = r.names[int(box.cls)]
            conf = float(box.conf)
            label = f"{cls_name} {conf:.2f}"

            # Color heuristic
            color = (0, 255, 0)  # green default
            if any(w in cls_name.lower() for w in ["crack", "damage"]):  # simplistic
                color = (255, 0, 0)   # red
            elif any(w in desc_lower for w in ["outdated", "old"]):
                color = (255, 165, 0)  # orange

            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
            draw.text((x1, y1-25), label, fill=color, font=font)

    return image

# â”€â”€ App Layout â”€â”€
st.title("ðŸ¡ AI Property Condition & Value Assessment")
st.caption("Opendoor-inspired prototype: Upload photos â†’ AI analyzes room quality, features, repairs â†’ suggests illustrative adjustments")

with st.sidebar:
    st.header("Settings")
    home_value = st.slider("Estimated home value ($)", 200_000, 2_000_000, 500_000, step=50_000)
    st.markdown("Adjustments are **illustrative** (Â±%) based on detected condition.")
    st.markdown("**Disclaimer**: Not financial advice. Inspired by Opendoor's AI assessments/RiskAI.")

uploaded_files = st.file_uploader("Upload 1â€“5 photos (kitchen, bath, exterior...)", type=["jpg", "jpeg", "png"], accept_multiple_files=True)

if uploaded_files:
    tab1, tab2 = st.tabs(["Per-Photo Details", "Aggregate Summary"])

    all_data = []
    progress = st.progress(0)

    for idx, file in enumerate(uploaded_files):
        progress.progress((idx + 1) / len(uploaded_files))
        image = Image.open(file).convert("RGB")

        with tab1:
            st.subheader(f"Photo {idx+1}")
            col1, col2 = st.columns(2)
            col1.image(image, use_column_width=True, caption="Original")

            # Room classification
            room_results = room_classifier(image)
            room = room_results[0]['label']
            room_conf = room_results[0]['score']
            col2.metric("Detected Room", room, f"{room_conf:.1%} conf")

            # YOLO + annotate
            yolo_results = yolo_model(image, verbose=False)
            annotated = annotate_image(image.copy(), yolo_results, "")
            col1.image(annotated, caption="YOLO26 Detection + Condition Highlights", use_column_width=True)

            # LLaVA analysis
            prompt = f"""Assess this {room} on quality scale 1â€“10. 
            Detail upgrades (modern appliances, premium materials), wear/outdated features, needed repairs (cracks, damage, mold, outdated fixtures). 
            Be objective, specific, and concise."""
            inputs = llava_processor(text=prompt, images=image, return_tensors="pt").to(device)
            with torch.inference_mode():
                output = llava_model.generate(**inputs, max_new_tokens=220, do_sample=False)
            llava_desc = llava_processor.decode(output[0], skip_special_tokens=True).strip()
            col2.markdown(f"**LLaVA-NeXT Analysis**\n{llava_desc}")

            # Adjustment
            adj_dollar, adj_pct, notes = compute_adjustment(room, llava_desc, yolo_results, home_value)
            all_data.append({
                "Photo": idx+1,
                "Room": room,
                "Quality (LLaVA)": llava_desc.split('\n')[0] if '\n' in llava_desc else llava_desc[:60],
                "Adjustment $": adj_dollar,
                "% Adj": adj_pct,
                "Notes": "; ".join(notes)
            })

            st.markdown("---")

    progress.empty()

    # â”€â”€ Aggregate â”€â”€
    with tab2:
        if all_data:
            df = pd.DataFrame(all_data)
            st.dataframe(df.style.format({
                "Adjustment $": "${:,.0f}",
                "% Adj": "{:.1f}%"
            }), use_container_width=True)

            total_adj = df["Adjustment $"].sum()
            avg_quality = df["% Adj"].mean()  # proxy

            cols = st.columns(3)
            cols[0].metric("Total Suggested Adjustment", f"${total_adj:,.0f}", delta_color="normal")
            cols[1].metric("Avg % Adjustment", f"{avg_quality:.1f}%")
            cols[2].metric("Photos Analyzed", len(uploaded_files))

            if total_adj > 0:
                st.success("Overall positive uplift likely from detected upgrades & condition.")
            elif total_adj < 0:
                st.warning("Potential deductions from wear, outdated features, or repairs.")
            else:
                st.info("Neutral net adjustment â€“ condition appears average.")

            # Export
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button("Download Summary CSV", csv, "property_assessment_summary.csv", "text/csv")

st.markdown("""
**How this could help at Opendoor**  
Automates condition flagging for faster Key Agent assessments, refines instant-offer pricing via RiskAI-like signals.  
Prototype only â€“ production would use fine-tuned models + diverse training data.
""")
```

### Final Touches & Deployment Tips
- **Test**: Use real listing photos from Zillow/Redfin (modern vs dated kitchens, cracked driveways).
- **Speed tweaks**: If LLaVA slow on free HF Spaces â†’ fallback to `Salesforce/blip-image-captioning-large` or lighter `llava-1.5-7b-hf`.
- **Deploy**: Hugging Face Spaces (GPU on for inference) or Streamlit Community Cloud. Add `config.toml` for custom theme.
- **README**: Include screenshots, Opendoor relevance, metrics (e.g., "Correct room ID in 92% test cases"), limitations (lighting bias, no true cost estimation).

This version feels professional, fast, visually engaging, and domain-smart. If you want PDF report generation (via `reportlab`), video walkthrough script, or fine-tune starter notebook â€“ just say! ðŸš€